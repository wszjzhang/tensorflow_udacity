{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Neural Networks\n",
    "\n",
    "___\n",
    "\n",
    "An artificial neural network is a predictive model motivated by the way the brain operates. Think of the brain as a collection of neurons wired together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptrons\n",
    "___\n",
    "\n",
    "The simplest neural network is the perceptron, which approximates a single neuron. It computes a weighted sum of its inputs and fires."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def step_function(x):\n",
    "    return 1 if x >= 0 else 0\n",
    "\n",
    "def perceptron_output(weights, bias, x):\n",
    "    \"\"\" returns 1 if the perceptron fires, 0 if not\"\"\"\n",
    "    calculation = np.dot(weights, x) + bias\n",
    "    return step_function(calculation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed-Forward Neural networks\n",
    "___\n",
    "\n",
    "Feed-Forward neural network entails an input layer, one or more hidden layers, and an output layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(t):\n",
    "    return 1/(1 + np.exp(-t))\n",
    "\n",
    "def neuron_output(weights, inputs):\n",
    "    return sigmoid(np.dot(weights, inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to train a neural network, we'll need to use calculus, and in order to use calculus, we need smooth functions. Sigmoid function is a good approximation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can represent a neural network as a list of layers, where each layer is just a list of the nuerons in that layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def feed_forward(neural_network, input_vector):\n",
    "    \"\"\"takes in a neural network\n",
    "        and returns the output from forward-propagating the input\"\"\"\n",
    "    outputs = []\n",
    "    \n",
    "    #process one layer at a time\n",
    "    for layer in neural_network:\n",
    "        input_with_bias = input_vector + [1]\n",
    "        output = [neuron_output(neuron, input_with_bias) for neuron in layer]\n",
    "        outputs.append(output)\n",
    "        input_vector = output\n",
    "    \n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 [9.3831466830067595e-14]\n",
      "0 1 [0.99999999999990585]\n",
      "1 0 [0.99999999999990585]\n",
      "1 1 [9.3831466830068276e-14]\n"
     ]
    }
   ],
   "source": [
    "xor_network = [#hidden layer\n",
    "                [[20, 20, -30],    # 'and' neuron\n",
    "                 [20, 20, -10]],   # 'or' neuron\n",
    "                # output layer\n",
    "                [[-60,60, -30]]]   # 2nd input but not first input\n",
    "\n",
    "for x in [0, 1]:\n",
    "    for y in [0, 1]:\n",
    "        print x, y, feed_forward(xor_network, [x, y])[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "\n",
    "___\n",
    "As usual we use data to train neutal networks. One popular approach is an algorithm called backpropagation that has similarities to the gradient descent algorithm. \n",
    "\n",
    "We adjust weights in the neural network using the algorithm:\n",
    "1. Run feed_forward on an input vector to produce the outputs of all the neurons in the net work.\n",
    "2. This results in an error for each output neuron -- the difference between its output and its target\n",
    "3. Compotes the gradient of this error as a function of the neuron's weights, and adjust its weights in the direction that most decreases the error.\n",
    "4. \"Propagate\" these output errors backward to infer errors for the hiden layer.\n",
    "5. Compute the gradients of these errors and adjust the weights of hidden layers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def backpropagate(network, input_vector, targets):\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
