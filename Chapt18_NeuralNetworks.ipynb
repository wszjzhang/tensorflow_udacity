{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Neural Networks\n",
    "\n",
    "___\n",
    "\n",
    "An artificial neural network is a predictive model motivated by the way the brain operates. Think of the brain as a collection of neurons wired together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib \n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptrons\n",
    "___\n",
    "\n",
    "The simplest neural network is the perceptron, which approximates a single neuron. It computes a weighted sum of its inputs and fires."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def step_function(x):\n",
    "    return 1 if x >= 0 else 0\n",
    "\n",
    "def perceptron_output(weights, bias, x):\n",
    "    \"\"\" returns 1 if the perceptron fires, 0 if not\"\"\"\n",
    "    calculation = np.dot(weights, x) + bias\n",
    "    return step_function(calculation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed-Forward Neural networks\n",
    "___\n",
    "\n",
    "Feed-Forward neural network entails an input layer, one or more hidden layers, and an output layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sigmoid(t):\n",
    "    return 1/(1 + np.exp(-t))\n",
    "\n",
    "def neuron_output(weights, inputs):\n",
    "    return sigmoid(np.dot(weights, inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to train a neural network, we'll need to use calculus, and in order to use calculus, we need smooth functions. Sigmoid function is a good approximation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can represent a neural network as a list of layers, where each layer is just a list of the nuerons in that layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def feed_forward(neural_network, input_vector):\n",
    "    \"\"\"takes in a neural network\n",
    "        and returns the output from forward-propagating the input\"\"\"\n",
    "    outputs = []\n",
    "    \n",
    "    #process one layer at a time\n",
    "    for layer in neural_network:\n",
    "        input_with_bias = input_vector + [1]\n",
    "        output = [neuron_output(neuron, input_with_bias) for neuron in layer]\n",
    "        outputs.append(output)\n",
    "        input_vector = output\n",
    "    \n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 [9.3831466830067595e-14]\n",
      "0 1 [0.99999999999990585]\n",
      "1 0 [0.99999999999990585]\n",
      "1 1 [9.3831466830068276e-14]\n"
     ]
    }
   ],
   "source": [
    "xor_network = [#hidden layer\n",
    "                [[20, 20, -30],    # 'and' neuron\n",
    "                 [20, 20, -10]],   # 'or' neuron\n",
    "                # output layer\n",
    "                [[-60,60, -30]]]   # 2nd input but not first input\n",
    "\n",
    "for x in [0, 1]:\n",
    "    for y in [0, 1]:\n",
    "        print x, y, feed_forward(xor_network, [x, y])[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "\n",
    "___\n",
    "As usual we use data to train neutal networks. One popular approach is an algorithm called backpropagation that has similarities to the gradient descent algorithm. \n",
    "\n",
    "We adjust weights in the neural network using the algorithm:\n",
    "1. Run feed_forward on an input vector to produce the outputs of all the neurons in the net work.\n",
    "2. This results in an error for each output neuron -- the difference between its output and its target\n",
    "3. Compotes the gradient of this error as a function of the neuron's weights, and adjust its weights in the direction that most decreases the error.\n",
    "4. \"Propagate\" these output errors backward to infer errors for the hiden layer.\n",
    "5. Compute the gradients of these errors and adjust the weights of hidden layers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def backpropagate(network, input_vector, targets):\n",
    "    \n",
    "    hidden_outputs, outputs = feed_forward(network, input_vector)\n",
    "    \n",
    "    # the output*(1-output) is from the derivative of sigmoid\n",
    "    output_deltas = [output * (1 - output)*(output - target) \n",
    "                    for output, target in zip(outputs, targets)]\n",
    "    \n",
    "    # adjust weights for output layer, one neuron at a time\n",
    "    for i, output_neuron in enumerate(network[-1]):\n",
    "        # ith output layer neuron \n",
    "        for j, hidden_output in enumerate(hidden_outputs + [1]):\n",
    "            # adjust jth weight based on this neuron's delta and ij input\n",
    "            output_neuron[j] -= output_deltas[i]*hidden_output\n",
    "         \n",
    "    # back-propagate errors to hidden layer\n",
    "    hidden_deltas = [hidden_output * (1 - hidden_output) *\n",
    "                    np.dot(output_deltas, [n[i] for n in output_layer])\n",
    "                    for i, hidden_output in enumerate(hidden_outputs)]\n",
    "        \n",
    "    # adjust weights for hidden layer, one neuron at atime\n",
    "    for i, hidden_neuron in enumerate(network[0]):\n",
    "        for j, input in enumerate(input_vector + [1]):\n",
    "            hidden_neuron[j] -= hidden_deltas[i] * input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "## Example: Defeating CAPTCHA \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define the neural network \n",
    "input element: 5*5 = 25;\n",
    "1 hidden layer with 5 neurons; \n",
    "output layer with 10 neurons; \n",
    "each neuron is a 10 element vector represent 10 digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#############################\n",
    "# neural network definition #\n",
    "#############################\n",
    "\n",
    "random.seed(0)\n",
    "input_size = 25 # each input is a vector of length 25\n",
    "num_hidden = 5  # 1 hidden layer, we'll have 5 neurons in the hidden layer\n",
    "output_size = 10 # we need 10 outputs for each input\n",
    "\n",
    "# each hidden neuron has one weight per input, plus a bias weight\n",
    "hidden_layer = [[random.random() for _ in xrange(input_size + 1)]\n",
    "               for __ in xrange(num_hidden)]\n",
    "\n",
    "# each output neuron has one weight per hidden neuron, plus a bias weight\n",
    "output_layer = [[random.random() for _ in xrange(num_hidden + 1)]\n",
    "               for __ in xrange(output_size)]\n",
    "\n",
    "# the network starts out with random weights\n",
    "network = [hidden_layer, output_layer]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.024521973898174929,\n",
       " 1.0576030623170791e-05,\n",
       " 1.3956363060379508e-10,\n",
       " 0.017400413633184932,\n",
       " 0.0010424389791314197,\n",
       " 6.8281060788083614e-10,\n",
       " 3.2721925578057628e-08,\n",
       " 0.96889849594860245,\n",
       " 1.1664852765015362e-08,\n",
       " 2.4400695314417383e-08]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###########################\n",
    "# neural network training #\n",
    "###########################\n",
    "\n",
    "zero = [1,1,1,1,1,\n",
    "        1,0,0,0,1,\n",
    "        1,0,0,0,1,\n",
    "        1,0,0,0,1,\n",
    "        1,1,1,1,1]\n",
    "one = [0,0,1,0,0,\n",
    "        0,0,1,0,0,\n",
    "        0,0,1,0,0,\n",
    "        0,0,1,0,0,\n",
    "        0,0,1,0,0]\n",
    "two = [1,1,1,1,1,\n",
    "        0,0,0,0,1,\n",
    "        1,1,1,1,1,\n",
    "        1,0,0,0,0,\n",
    "        1,1,1,1,1]\n",
    "three = [1,1,1,1,1,\n",
    "        0,0,0,0,1,\n",
    "        1,1,1,1,1,\n",
    "        0,0,0,0,1,\n",
    "        1,1,1,1,1]\n",
    "four = [1,0,0,0,1,\n",
    "        1,0,0,0,1,\n",
    "        1,1,1,1,1,\n",
    "        0,0,0,0,1,\n",
    "        0,0,0,0,1]\n",
    "five = [1,1,1,1,1,\n",
    "        1,0,0,0,0,\n",
    "        1,1,1,1,1,\n",
    "        0,0,0,0,1,\n",
    "        1,1,1,1,1]\n",
    "six = [1,1,1,1,1,\n",
    "        1,0,0,0,0,\n",
    "        1,1,1,1,1,\n",
    "        1,0,0,0,1,\n",
    "        1,1,1,1,1]\n",
    "seven = [1,1,1,1,1,\n",
    "        0,0,0,0,1,\n",
    "        0,0,0,0,1,\n",
    "        0,0,0,0,1,\n",
    "        0,0,0,0,1]\n",
    "eight = [1,1,1,1,1,\n",
    "        1,0,0,0,1,\n",
    "        1,1,1,1,1,\n",
    "        1,0,0,0,1,\n",
    "        1,1,1,1,1]\n",
    "nine = [1,1,1,1,1,\n",
    "        1,0,0,0,1,\n",
    "        1,1,1,1,1,\n",
    "        0,0,0,0,1,\n",
    "        1,1,1,1,1]\n",
    "inputs = [zero, one, two, three, four, five, six, seven, eight, nine]\n",
    "targets = [[1 if i == j else 0 for i in xrange(10)]\n",
    "          for j in xrange(10)]\n",
    "\n",
    "# 10,000 iterations seems enough to converge\n",
    "for _ in xrange(10000):\n",
    "    for input_vector, target_vector in zip(inputs, targets):\n",
    "        backpropagate(network, input_vector, target_vector)\n",
    "\n",
    "\n",
    "def predict(input):\n",
    "    return feed_forward(network, input)[-1]\n",
    "predict(inputs[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0024748891253380226,\n",
       " 1.3500399517235218e-14,\n",
       " 7.0038209359308007e-10,\n",
       " 1.0479803347849043e-05,\n",
       " 2.1715577993551197e-09,\n",
       " 0.04535706395954088,\n",
       " 0.00049215604435028895,\n",
       " 2.2383093862239574e-06,\n",
       " 0.95369325528868609,\n",
       " 0.25686602211802206]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict([1,1,1,1,1,\n",
    "        1,0,0,0,1,\n",
    "        1,1,1,1,1,\n",
    "        1,1,0,0,1,\n",
    "        1,1,1,1,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPYAAAD7CAYAAABZjGkWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10580a810>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "weights = network[0][0]\n",
    "abs_weights = map(abs, weights)\n",
    "grid = [abs_weights[row:(row+5)]\n",
    "       for row in range(0,25,5)]\n",
    "\n",
    "ax = plt.gca()\n",
    "\n",
    "ax.imshow(grid,\n",
    "         cmap=matplotlib.cm.binary,\n",
    "         interpolation='none')\n",
    "\n",
    "def patch(x, y, hatch, color):\n",
    "    return matplotlib.patches.Rectangle((x-0.5, y-0.5), 1, 1,\n",
    "                                hatch = hatch, fill=False, color=color)\n",
    "\n",
    "for i in range(5):\n",
    "    for j in range(5):\n",
    "        if weights[5*i+j] < 0:\n",
    "            ax.add_patch(patch(j, i, '/', \"white\"))\n",
    "            ax.add_patch(patch(j, i, '\\\\', \"black\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
